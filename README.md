# Multi-Cloud_Projects
This is a Repo to store Files, Code, and Lab documents for Multi-Cloud based projects.

Steps to implement Hands-on Project - Mission 1
Amazon Web Services
‚Ä¢	Access AWS console and go to IAM service
‚Ä¢	Under Access management, Click in "Users", then "Add user" and create a programmatic user "terraform-en-1"
‚Ä¢	On Set permissions, click on "Attach existing policies directly" button.
‚Ä¢	Select AmazonS3FullAccess.
‚Ä¢	Click on Next: Tags
‚Ä¢	Click on Next: Review
‚Ä¢	Click on Create user
‚Ä¢	Click on Download .csv
‚Ä¢	After download, rename .csv to accessKeys.csv
Google Cloud Platform (GCP)
‚Ä¢	**CLICK HERE to download the hands-on files**.
‚Ä¢	Access GCP Console and open Cloud Shell
‚Ä¢	Upload accessKeys.csv and .zip hands-on file to GCP Cloud Shell
‚Ä¢	Hands-on files preparation
mkdir mission1_en
mv mission1.zip mission1_en
cd mission1_en
unzip mission1.zip
mv ~/accessKeys.csv mission1/en
cd mission1/en
chmod +x *.sh
‚Ä¢	Run the following commands to prepare AWS and GCP environment. Authorize when asked.
./aws_set_credentials.sh accessKeys.csv
gcloud config set project <project_id>
‚Ä¢	Execute the command below
./gcp_set_project.sh
‚Ä¢	Enable the Container Registry API, Kubernetes Engine API and the Cloud SQL API
gcloud services enable containerregistry.googleapis.com 
gcloud services enable container.googleapis.com 
gcloud services enable sqladmin.googleapis.com 
IMPORTANT (DO NOT SKIP):
‚Ä¢	Before executing the Terraform commands, open the Google Editor and update the file tcb_aws_storage.tf replacing the bucket name with an unique name (AWS requires unique bucket names).
o	Open the tcb_aws_storage.tf using Google Editor
o	On line 4 of the file tcb_aws_storage.tf: 
ÔÇß	Replace xxxx with your name initials plus two random numbers: Example: luxxy-covid-testing-system-pdf-en-jr29
‚Ä¢	Run the following commands to finish provision infrastructure steps
cd ~/mission1_en/mission1/en/terraform/

terraform init
terraform plan
terraform apply
‚Ä¢	Once the CloudSQL instance is provisioned, access the Cloud SQL service
‚Ä¢	Click on your Cloud SQL instance.
‚Ä¢	On the left side, under Primary Instance, click on Connections.
‚Ä¢	Under Instance IP assignment, enable Private IP. 
o	Click Set up Connection and Use an automatically allocated IP range in your network.
o	Click Continue
o	Click Create Connection and wait a minutes.
‚Ä¢	Under Associated networking, select "Default"
‚Ä¢	Under Authorized Networks, click "Add Network".
‚Ä¢	Under New Network, enter the following information: 
o	Name: Public Access (For testing purposes only)
o	Network:** 0.0.0.0/0
o	Click Done.
PS: For production environments, it is recommended to use only the Private Network for database access. ‚ö†Ô∏è Never grant public network access (0.0.0.0/0) to production databases.
‚Ä¢	After that, click on Save and wait to conclude the update.


Excited to try my hand at building Infrastructure in the cloud using Terraform...
I am gaining tremendous insight and knowledge of cloud automation with this 
Multi-Cloud integration with AWS and Google Cloud thanks to the 
@thecloudbootcamp  #clouds #cloudcomputing #aws #googlecloud #azure #terraform #kubernets


Steps to implement Hands-on Project - Mission 2
Amazon Web Services
‚Ä¢	Access AWS console and go to IAM service
‚Ä¢	Under Access management, Click in "Users", then "Add user" and create a programmatic user "luxxy-covid-testing-system-en-app1"
‚Ä¢	On Set permissions, click on "Attach existing policies directly" button.
‚Ä¢	Select AmazonS3FullAccess.
‚Ä¢	Click on Next: Tags
‚Ä¢	Click on Next: Review
‚Ä¢	Click on Create user
‚Ä¢	Click on Download .csv
‚Ä¢	After download, rename .csv to luxxy-covid-testing-system-en-app1.csv
Google Cloud Platform (GCP)
‚Ä¢	Navigate to Cloud SQL instance and create a new user app with password welcome123456 on Cloud SQL MySQL database
‚Ä¢	Connect to Google Cloud Shell
‚Ä¢	Download the mission2 files
cd
mkdir mission2_en
cd mission2_en
wget <https://objectstorage.us-ashburn-1.oraclecloud.com/p/t1RPAY7hbbdaSj9fhLJhYetCsFe5CEKufO56vb1l2_6FVz35PJzIukvTJKhRYKGs/n/idqfa2z2mift/b/eventos-thecloudbootcamp/o/ICP2/mission2.zip>
unzip mission2.zip
‚Ä¢	Connect to MySQL DB running on Cloud SQL (once it prompts for the password, provide welcome123456)
mysql --host=<public_ip_cloudsql> --port=3306 -u app -p
‚Ä¢	Once you're connected to the database instance, create the products table for testing purposes
use dbcovidtesting;
source ~/mission2_en/mission2/en/db/create_table.sql;
show tables;
exit;
‚Ä¢	Build the Docker image and push it to Google Container Registry. Please replace the <PROJECT_ID> with your My First Project ID
cd ~/mission2_en/mission2/en/app
gcloud builds submit --tag gcr.io/<PROJECT_ID>/luxxy-covid-testing-system-app-en
‚Ä¢	Open the Cloud Editor and edit the Kubernetes deployment file (luxxy-covid-testing-system.yaml) and update the variables below in red with your <PROJECT_ID> on the Google Container Registry path, AWS Bucket name, AWS Keys (from luxxy-covid-testing-system-en-app1.csv) and Cloud SQL Database Private IP.
cd ~/mission2/en/kubernetes
luxxy-covid-testing-system.yaml

				image: gcr.io/**<PROJECT_ID>/**luxxy-covid-testing-system-app-en:latest
...
				- name: AWS_BUCKET
          value: "**luxxy-covid-testing-system-pdf-en-xxxx**"
        - name: S3_ACCESS_KEY
          value: "**xxxxxxxxxxxxxxxxxxxxx**"
        - name: S3_SECRET_ACCESS_KEY
          value: "**xxxxxxxxxxxxxxxxxxxx**"
        - name: DB_HOST_NAME
          value: "**172.21.0.3**"
‚Ä¢	Connect to the GKE (Google Kubernetes Engine) cluster via Console (follow the video)
‚Ä¢	Deploy the application Pharma Store in the Cluster
cd ~/mission2_en/mission2/en/kubernetes
kubectl apply -f luxxy-covid-testing-system.yaml
‚Ä¢	Get the Public IP and test the application (CLICK HERE to download COVID-19 Testing result sample)
‚Ä¢	You should see the app up & running! Congrats! üéâ

Steps to implement Hands-on Project - Mission 3
Google Cloud Platform - Database Migration steps
‚Ä¢	Connect to Google Cloud Shell
‚Ä¢	Download the dump
cd
mkdir mission3_en
cd mission3_en
wget <https://objectstorage.us-ashburn-1.oraclecloud.com/p/tvk3j_I8Uj0tTvPE2A2JDWCF_TZpmw0vDIiHkD8HKzGYxZsZtWXBfaURIZ4djW1k/n/idqfa2z2mift/b/eventos-thecloudbootcamp/o/ICP2/mission3.zip>
unzip mission3.zip
‚Ä¢	Connect to Cloud SQL MySQL database instance
mysql --host=<public_ip_address> --port=3306 -u app -p
‚Ä¢	Import the dump on Cloud SQL
use dbcovidtesting;
source ~/mission3_en/mission3/en/db/db_dump.sql
‚Ä¢	Check if the data got imported correctly
SELECT * FROM records;
Amazon Web Services - PDF Files Migration steps
‚Ä¢	Connect to the AWS Cloud Shell
‚Ä¢	Download the PDF files
cd
mkdir mission3_en
cd mission3_en
wget <https://objectstorage.us-ashburn-1.oraclecloud.com/p/tvk3j_I8Uj0tTvPE2A2JDWCF_TZpmw0vDIiHkD8HKzGYxZsZtWXBfaURIZ4djW1k/n/idqfa2z2mift/b/eventos-thecloudbootcamp/o/ICP2/mission3.zip>
unzip mission3.zip
‚Ä¢	Sync PDF Files with your AWS S3 used for COVID-19 Testing Status System. Replace the bucket name with yours.
cd mission3/en/pdf_files
aws s3 sync . s3://luxxy-covid-testing-system-pdf-en-**xxxx**
‚Ä¢	Test the application. Upon migrating the data and files, you should be able to see the entries under ‚ÄúView Guest Results‚Äù page.
 
Congratulations! You have migrated an "on-premises" application & database to a MultiCloud Architecture!
How to delete the Multiple Cloud Providers resources
First step - Empty your bucket
‚Ä¢	Access the AWS Console and open S3 Service.
‚Ä¢	Select your bucket and click on Empty.
‚Ä¢	To confirm deletion, type permanently delete in the text input field.
‚Ä¢	Click on Empty.
First step concluded! üéâ
Second step - Set the instance deletion_protection from true to false
‚Ä¢	Open the Google Editor, and access the following path: mission1_en/mission1/en/terraform/. Now, open the tcb_gcp_database.tf file
‚Ä¢	On line 11 of the tcb_gcp_database.tf file:
o	Replace from deletion_protection = ‚Äútrue‚Äù to deletion_protection = ‚Äúfalse‚Äù Example: 
ÔÇß	First version: deletion_protection = "true‚Äù
ÔÇß	Updated version: deletion_protection = "false‚Äù
‚Ä¢	Change from Editor to Terminal
‚Ä¢	Run the commands to apply the terraform file changes:
cd ~/mission1_en/mission1/en/terraform/

terraform apply
	***Enter a value: yes***
Now, you‚Äôre ready to delete all the resources with the terraform destroy. üöÄ
‚Ä¢	Run the following command:
terraform destroy
	***Enter a value: yes***
 
Congratulations! All resources were deleted from multiple Cloud Providers with just a command! üöÄ‚úÖ
